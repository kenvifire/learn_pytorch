{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import time\n",
    "import torchvision.datasets.mnist as ds\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_file_allowed_extension(filename, extensions):\n",
    "    \"\"\"Checks if a file is an allowed extension.\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "        extensions (iterable of strings): extensions to consider (lowercase)\n",
    "    Returns:\n",
    "        bool: True if the filename ends with one of given extensions\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    return any(filename_lower.endswith(ext) for ext in extensions)\n",
    "\n",
    "def make_dataset(dir, class_to_idx, extensions):\n",
    "    images = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    for target in sorted(class_to_idx.keys()):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in sorted(fnames):\n",
    "                if has_file_allowed_extension(fname, extensions):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "class TinyMindDataset(Dataset):\n",
    "    \"\"\"TInymind contest dataset\"\"\"\n",
    "    def __init__(self, root_dir, extensions, transform=None):\n",
    "        classes, class_to_idx = self._find_classes(root_dir)\n",
    "        self.files = make_dataset(root_dir, class_to_idx, extensions)\n",
    "        self.transform = transform\n",
    "        self.extensioons = extensions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.files[idx]\n",
    "        img = Image.open(path).resize((64,64))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, target\n",
    "    \n",
    "    def _find_classes(self, dir):\n",
    "        classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "tiny_dataset = TinyMindDataset(root_dir='/home/hannahzhang/tinymind/train/', extensions=['jpg'], transform=transform)\n",
    "data_size = len(tiny_dataset)\n",
    "split = (int)(data_size / 10)\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(tiny_dataset, batch_size=10, shuffle=False, sampler=train_sampler, num_workers=2)\n",
    "valid_dataloader = DataLoader(tiny_dataset, batch_size=10, shuffle=False, sampler=valid_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "# vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for param in vgg.features.parameters(): param.requires_grad = False\n",
    "# vgg.features[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "# vgg.classifier[6].out_features = 100\n",
    "# vgg_optimizer = optim.SGD(vgg.classifier.parameters(), lr=0.01, momentum=.65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size = 3)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size = 3)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(1152, 100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss= 4.623625755310059\n",
      "Got 31 / 4000 correct (0.78)\n",
      "\n",
      "Iteration 100, loss= 4.576729774475098\n",
      "Got 43 / 4000 correct (1.07)\n",
      "\n",
      "Iteration 200, loss= 4.603630542755127\n",
      "Got 54 / 4000 correct (1.35)\n",
      "\n",
      "Iteration 300, loss= 4.580783367156982\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "\n",
    "def check_accuracy(loader, model): \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "            \n",
    "def train(model, optimizer, loader_train, loader_val, epochs=1):\n",
    "    model = model.to(device=device)\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Iteration {}, loss= {}'.format(t, loss.item()))\n",
    "                check_accuracy(loader_val, model)\n",
    "                print()\n",
    "    \n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "train(model, optimizer, train_dataloader, valid_dataloader, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train(vgg, vgg_optimizer, train_dataloader, valid_dataloader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/hannahzhang/learn_pytorch/tiny\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
